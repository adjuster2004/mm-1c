import os
import sys
import json
import re
import io
import pandas as pd
import requests
import threading
import asyncio
from datetime import datetime, date
from mattermostdriver import Driver
import jinja2

# --- –ò–ú–ü–û–†–¢ –ú–û–î–£–õ–Ø TEAMS ---
try:
    import teams
except ImportError:
    print("‚ö†Ô∏è –ú–æ–¥—É–ª—å teams.py –Ω–µ –Ω–∞–π–¥–µ–Ω. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –ª–∏–¥–æ–≤ –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
    teams = None

# --- –ó–ê–ì–†–£–ó–ö–ê –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò ---
def get_env(key, default=None, required=False):
    val = os.getenv(key, default)
    if required and not val:
        print(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ –∑–∞–¥–∞–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è {key}")
        sys.exit(1)
    return val

MM_URL = get_env("MM_URL", required=True)
MM_TOKEN = get_env("MM_TOKEN", required=True)
MM_PORT = int(get_env("MM_PORT", "443"))
MM_SCHEME = get_env("MM_SCHEME", "https")
MM_TARGET_CHANNEL_ID = get_env("MM_TARGET_CHANNEL_ID", required=True)

JIRA_DOMAIN = get_env("JIRA_DOMAIN", required=True)
JIRA_TOKEN = get_env("JIRA_TOKEN", required=True)
AUTH_METHOD = get_env("JIRA_AUTH_METHOD", "Bearer")
VERIFY_SSL = get_env("VERIFY_SSL", "False").lower() in ('true', '1', 't')

if not VERIFY_SSL:
    requests.packages.urllib3.disable_warnings()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∫—ç—à–∏
JIRA_LOOKUP_CACHE = {}
JIRA_KEY_CACHE = {}

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---
def get_headers():
    if AUTH_METHOD == "Bearer":
        return {"Authorization": f"Bearer {JIRA_TOKEN}", "Content-Type": "application/json"}
    return {"Cookie": f"JSESSIONID={JIRA_TOKEN}", "Content-Type": "application/json"}

def get_all_jira_users():
    print("‚è≥ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π Jira...", flush=True)
    users = []
    search_queries = ['.', '@', '']
    for query in search_queries:
        print(f"üîé –ü–æ–∏—Å–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π API Jira –ø–æ –º–∞—Å–∫–µ: '{query}'", flush=True)
        temp_users = []
        start_at = 0
        while True:
            url = f"https://{JIRA_DOMAIN}/rest/api/2/user/search"
            params = {"username": query, "startAt": start_at, "maxResults": 1000, "includeInactive": "false"}
            try:
                resp = requests.get(url, headers=get_headers(), params=params, verify=VERIFY_SSL, timeout=(10, 30))
                if resp.status_code != 200: break
                chunk = resp.json()
                if not chunk: break
                temp_users.extend(chunk)
                if len(chunk) < 1000: break
                start_at += len(chunk)
            except: break
        if temp_users:
            users = temp_users
            break

    lookup_map = {}
    key_map = {}
    for u in users:
        login = u.get('name')
        key = u.get('key')
        d_name = u.get('displayName')
        if not key: key = login
        user_obj = {'login': login, 'key': key, 'displayName': d_name}
        if key: key_map[key] = user_obj
        if d_name:
            lookup_map[d_name.lower()] = user_obj
            parts = d_name.split()
            if len(parts) == 2:
                rev_name = f"{parts[1]} {parts[0]}"
                lookup_map[rev_name.lower()] = user_obj
        if login: lookup_map[login.lower()] = user_obj
    return lookup_map, key_map

def update_progress_message(post_id, channel_id, message):
    try:
        driver.posts.update_post(post_id, options={'id': post_id, 'channel_id': channel_id, 'message': message})
    except Exception as e: print(f"Status update error: {e}", flush=True)

def parse_tempo_date(date_val):
    if not date_val: return None
    s = str(date_val).lower().strip().split('t')[0]
    try:
        if '-' in s: return datetime.strptime(s, '%Y-%m-%d').date()
        if '/' in s: return datetime.strptime(s, '%Y/%m/%d').date()
        if '.' in s: return datetime.strptime(s, '%d.%m.%Y').date()
    except: pass
    return None

def get_team_rank(team_name):
    tn = team_name.lower()
    if tn == 'other': return (99, 0, tn)
    if 'arch-team' in tn: return (1, 0, tn)
    if 'change-team' in tn: return (2, 0, tn)
    if 'stream' in tn:
        match = re.search(r'stream(\d+)', tn)
        num = int(match.group(1)) if match else 999
        return (3, num, tn)
    return (4, 0, tn)

def get_tempo_teams_assignments(report_start_date, report_end_date):
    print("‚è≥ –ê–Ω–∞–ª–∏–∑ –∫–æ–º–∞–Ω–¥ Tempo...", flush=True)
    try:
        resp = requests.get(f"https://{JIRA_DOMAIN}/rest/tempo-teams/2/team", headers=get_headers(), verify=VERIFY_SSL, timeout=30)
        if resp.status_code != 200: return {}
        all_teams = resp.json()
    except: return {}

    target_teams = []
    pattern = re.compile(r"^(stream.*-team|change-team|arch-team)$", re.IGNORECASE)
    for team in all_teams:
        if pattern.match(team.get("name", "")): target_teams.append(team)

    user_team_map = {}
    for team in target_teams:
        try:
            m_resp = requests.get(f"https://{JIRA_DOMAIN}/rest/tempo-teams/2/team/{team.get('id')}/member", headers=get_headers(), verify=VERIFY_SSL, timeout=30)
            if m_resp.status_code == 200:
                for m in m_resp.json():
                    jira_key = m.get("member", {}).get("key")
                    if not jira_key: continue
                    ms = m.get("membership", {})
                    d_from = parse_tempo_date(ms.get('dateFromANSI') or ms.get('dateFrom')) or date(2000, 1, 1)
                    d_to = parse_tempo_date(ms.get('dateToANSI') or ms.get('dateTo')) or date(2099, 12, 31)
                    if d_from <= report_end_date and d_to >= report_start_date:
                        user_team_map[jira_key] = team.get("name")
        except: pass
    return user_team_map

def fetch_tempo_worklogs_for_users(start_date, end_date, worker_ids, progress_callback=None):
    all_worklogs = []
    chunks = [worker_ids[i:i + 25] for i in range(0, len(worker_ids), 25)]
    for i, chunk_workers in enumerate(chunks):
        if progress_callback: progress_callback(i + 1, len(chunks))
        payload = {"from": start_date.strftime("%Y-%m-%d"), "to": end_date.strftime("%Y-%m-%d"), "worker": chunk_workers}
        try:
            resp = requests.post(f"https://{JIRA_DOMAIN}/rest/tempo-timesheets/4/worklogs/search", headers=get_headers(), json=payload, verify=VERIFY_SSL, timeout=90)
            if resp.status_code == 200: all_worklogs.extend(resp.json().get('results', []) if isinstance(resp.json(), dict) else resp.json())
        except: pass
    return all_worklogs

def check_name_match(jira_name, excel_name):
    if not jira_name or not excel_name: return False
    j_parts = [p for p in str(jira_name).lower().replace('.', ' ').split() if len(p)>1]
    e_parts = [p for p in str(excel_name).lower().replace('.', ' ').split() if len(p)>1]
    return not set(j_parts).isdisjoint(set(e_parts))

def extract_period_from_excel(df_head):
    dates = []
    for _, row in df_head.iterrows():
        matches = re.findall(r'\d{2}\.\d{2}\.\d{4}', str(row.values))
        for m in matches:
            try: dates.append(datetime.strptime(m, '%d.%m.%Y').date())
            except: pass
    if len(dates) >= 2: return min(dates), max(dates)
    return None, None

# --- –¢–Ø–ñ–ï–õ–ê–Ø –†–ê–ë–û–¢–ê –í –ü–û–¢–û–ö–ï ---
def worker_process_file(file_id, channel_id, root_id):
    print(f"[THREAD] –ü–æ—Ç–æ–∫ –∑–∞–ø—É—â–µ–Ω –¥–ª—è —Ñ–∞–π–ª–∞ {file_id}", flush=True)
    status_post_id = None
    try:
        status_post = driver.posts.create_post(options={'channel_id': channel_id, 'message': '‚è≥ –§–∞–π–ª –≤ –æ—á–µ—Ä–µ–¥–∏...', 'root_id': root_id})
        status_post_id = status_post['id']
    except: pass

    def update_status_text(text):
        if status_post_id: update_progress_message(status_post_id, channel_id, text)

    try:
        # 1. –°–ö–ê–ß–ò–í–ê–ù–ò–ï –§–ê–ô–õ–ê
        raw_file_resp = requests.get(f"{MM_SCHEME}://{MM_URL}/api/v4/files/{file_id}", headers={"Authorization": f"Bearer {MM_TOKEN}"}, verify=VERIFY_SSL, timeout=60)
        if raw_file_resp.status_code != 200: return
        file_bytes = io.BytesIO(raw_file_resp.content)

        update_status_text("‚è≥ –ß–∏—Ç–∞—é Excel...")
        try: df_raw = pd.read_excel(file_bytes, header=None)
        except Exception as e:
            update_status_text(f"‚ùå –û—à–∏–±–∫–∞ Excel: {e}")
            return

        start_date, end_date = extract_period_from_excel(df_raw.head(20))
        if not start_date:
            update_status_text("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –ø–µ—Ä–∏–æ–¥ –¥–∞—Ç.")
            return

        # 2. –ü–ê–†–°–ò–ù–ì EXCEL
        header_row_idx = None
        name_col_idx = None
        hours_col_idx = None
        absence_cols = []

        for idx, row in df_raw.iterrows():
            for c, val in enumerate(row):
                if str(val).lower().startswith("—Ñ–∞–º–∏–ª–∏—è"):
                    header_row_idx, name_col_idx = idx, c
                    break
            if header_row_idx is not None: break

        if header_row_idx is None:
            update_status_text("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ '–§–∞–º–∏–ª–∏—è'.")
            return

        for c in range(len(df_raw.columns)-1, -1, -1):
            if pd.notna(df_raw.iloc[len(df_raw)//2, c]):
                try:
                    float(str(df_raw.iloc[len(df_raw)//2, c]).replace(',','.'))
                    hours_col_idx = c
                    break
                except: pass

        for r in range(header_row_idx, min(header_row_idx+3, len(df_raw))):
            for c in range(len(df_raw.columns)):
                if c > name_col_idx and "–∫–æ–¥" in str(df_raw.iloc[r, c]).lower() and c not in absence_cols:
                    absence_cols.append(c)

        excel_data = []
        target_jira_keys = set()
        unique_users = {v['key']: v for v in JIRA_LOOKUP_CACHE.values()}.values()

        for i in range(header_row_idx + 1, len(df_raw)):
            row = df_raw.iloc[i]
            raw_name = row[name_col_idx]
            if pd.isna(raw_name) or len(str(raw_name)) < 2: continue
            if any(k in str(raw_name).lower() for k in ["–∏—Ç–æ–≥–æ", "–ø–æ–¥–ø–∏—Å—å", "–¥–æ–ª–∂–Ω–æ—Å—Ç—å", "–ø—Ä–æ—Ñ–µ—Å—Å–∏—è"]): continue

            clean_name = str(raw_name).split('\n')[0].split('(')[0].strip()
            hours = 0
            absences = set()

            for offset in range(4):
                if i + offset >= len(df_raw): break
                if hours_col_idx:
                    try:
                        h = float(str(df_raw.iloc[i+offset, hours_col_idx]).replace(',', '.').replace(' ', ''))
                        if h > hours: hours = h
                    except: pass

                for ac in absence_cols:
                    val = df_raw.iloc[i+offset, ac]
                    if pd.notna(val):
                        s = str(val).strip()
                        if s and not s.isdigit() and len(s) < 5 and s.upper() != '–Ø': absences.add(s)

            if hours > 0 or absences:
                found_u = next((u for u in unique_users if check_name_match(u['displayName'], clean_name)), None)
                if found_u: target_jira_keys.add(found_u['key'])
                excel_data.append({"name_1c": clean_name, "hours_1c": hours, "jira_user": found_u, "absences": sorted(list(absences))})

        # 3. –ü–û–õ–£–ß–ï–ù–ò–ï –î–ê–ù–ù–´–•
        update_status_text("‚è≥ –û–ø—Ä–µ–¥–µ–ª—è—é –∫–æ–º–∞–Ω–¥—ã...")
        team_mapping = get_tempo_teams_assignments(start_date, end_date)

        # --- –ó–ê–ì–†–£–ó–ö–ê –õ–ò–î–û–í –ò–ó CONFLUENCE ---
        leads_mapping = {}
        if teams:
            update_status_text("‚è≥ –ü–æ–ª—É—á–∞—é –ª–∏–¥–æ–≤ –∏–∑ Confluence...")
            leads_mapping = teams.fetch_team_leads_mapping()

        tempo_agg = {}
        if target_jira_keys:
            def pc(c, t): update_status_text(f"‚è≥ Tempo... {int(c/t*100)}%")
            logs = fetch_tempo_worklogs_for_users(start_date, end_date, list(target_jira_keys), pc)
            for l in logs:
                wid = l.get('worker')
                tempo_agg[wid] = tempo_agg.get(wid, 0) + l.get('timeSpentSeconds', 0)

        # 4. –°–ë–û–†–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–ê
        update_status_text("‚è≥ –§–æ—Ä–º–∏—Ä—É—é –æ—Ç—á–µ—Ç...")
        report_rows = []
        for r in excel_data:
            t_sec = 0
            j_name, j_key, t_name = "‚Äî", "‚Äî", "Other"

            if r['jira_user']:
                j_name = r['jira_user']['displayName']
                j_key = r['jira_user']['key']
                j_login = r['jira_user']['login']
                t_sec = tempo_agg.get(j_key) or tempo_agg.get(j_login) or 0
                if j_key in team_mapping: t_name = team_mapping[j_key]

            t_hours = round(t_sec / 3600, 2)
            diff = round(t_hours - r['hours_1c'], 2)

            status = "‚úÖ OK"
            if abs(diff) > 4: status = "‚ö†Ô∏è –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ"
            if j_name == "‚Äî": status = "‚ùì –ù–µ –Ω–∞–π–¥–µ–Ω –≤ Jira"

            report_rows.append({
                "Team": t_name, "–°–æ—Ç—Ä—É–¥–Ω–∏–∫ (1C)": r['name_1c'], "–°–æ—Ç—Ä—É–¥–Ω–∏–∫ (Jira)": j_name,
                "Jira Key": j_key, "Link": f"https://{JIRA_DOMAIN}/secure/Tempo.jspa#/my-work/timesheet?worker={j_key}&viewType=TIMESHEET" if j_key != "‚Äî" else None,
                "–ß–∞—Å—ã 1–°": r['hours_1c'], "–ù–µ—è–≤–∫–∏ (1–°)": ", ".join(r['absences']), "–ß–∞—Å—ã Tempo": t_hours,
                "–†–∞–∑–Ω–∏—Ü–∞": diff, "–°—Ç–∞—Ç—É—Å": status
            })

        df = pd.DataFrame(report_rows)
        df['SortRank'] = df['Team'].apply(lambda x: get_team_rank(x)[0])
        df['SortNum'] = df['Team'].apply(lambda x: get_team_rank(x)[1])
        df = df.sort_values(by=['SortRank', 'SortNum', 'Team', '–°–æ—Ç—Ä—É–¥–Ω–∏–∫ (1C)'])

        # 5. –ì–ï–ù–ï–†–ê–¶–ò–Ø –§–ê–ô–õ–ê
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            def hl(row):
                codes = [c.strip().upper() for c in (str(row['–ù–µ—è–≤–∫–∏ (1–°)']) or "").split(',')]
                return ['background-color: #ffffcc'] * len(row) if any(c != '–í' and c for c in codes) else [''] * len(row)

            df.drop(columns=['SortRank', 'SortNum', 'Link']).style.apply(hl, axis=1).to_excel(writer, index=False, sheet_name='Sverka')
            wb = writer.book
            ws = writer.sheets['Sverka']
            l_fmt = wb.add_format({'font_color': 'blue', 'underline': 1})
            ws.set_column(0, 0, 20); ws.set_column(1, 2, 25); ws.set_column(3, 3, 20)

            for idx, row in df.iterrows():
                if row['Link']:
                    rn = df.index.get_loc(idx) + 1
                    ws.write_url(rn, 3, row['Link'], l_fmt, string=row['Jira Key'])
        output.seek(0)

        # 6. –û–¢–ü–†–ê–í–ö–ê –í –ß–ê–¢
        print("[THREAD] –û—Ç–ø—Ä–∞–≤–∫–∞...", flush=True)
        msg_lines = [f"üìä **–ò—Ç–æ–≥ —Å–≤–µ—Ä–∫–∏**", f"‚ÑπÔ∏è {start_date} ‚Äî {end_date} (–°–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤: {len(target_jira_keys)})", "", ":al:  - Tempo > 1C", ":bangbang:  - —Å–º —Ç–∞–±–µ–ª—å", "üîª  -  1C > Tempo", ""]

        teams_with_issues = set() # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–º–∞–Ω–¥—ã —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏

        unique_teams = []
        for t in df['Team']:
            if t not in unique_teams: unique_teams.append(t)

        for team in unique_teams:
            team_df = df[df['Team'] == team]
            bad = team_df[team_df['–°—Ç–∞—Ç—É—Å'].str.contains("‚ö†Ô∏è")]
            if bad.empty:
                msg_lines.append(f"üìÅ **{team}**: ‚úÖ –í—Å–µ –û–ö")
            else:
                teams_with_issues.add(team) # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—É—é –∫–æ–º–∞–Ω–¥—É
                msg_lines.append(f"üìÅ **{team}**: ‚ö†Ô∏è –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π: **{len(bad)}**")
                for _, r in bad.iterrows():
                    icon = "üîª" if r['–†–∞–∑–Ω–∏—Ü–∞'] < 0 else ":al:"
                    bang = " :bangbang:" if any(c != '–í' and c for c in [x.strip().upper() for x in str(r['–ù–µ—è–≤–∫–∏ (1–°)']).split(',')]) else ""
                    abs_s = f" ({r['–ù–µ—è–≤–∫–∏ (1–°)']})" if r['–ù–µ—è–≤–∫–∏ (1–°)'] else ""
                    msg_lines.append(f"  - **{r['–°–æ—Ç—Ä—É–¥–Ω–∏–∫ (1C)']}**: 1C=`{r['–ß–∞—Å—ã 1–°']}` | Tempo=`{r['–ß–∞—Å—ã Tempo']}` | Diff: **{r['–†–∞–∑–Ω–∏—Ü–∞']}** {icon}{bang}{abs_s}")

        not_found = df[df['–°—Ç–∞—Ç—É—Å'].str.contains("‚ùì")]
        if not not_found.empty:
            msg_lines.append(f"\n‚ùì **–ù–µ –Ω–∞–π–¥–µ–Ω—ã ({len(not_found)}):**\n_{', '.join(not_found['–°–æ—Ç—Ä—É–¥–Ω–∏–∫ (1C)'].head(5))}_...")

        up_file = driver.files.upload_file(channel_id=channel_id, files={'files': ('report.xlsx', output)})
        driver.posts.delete_post(status_post_id)
        driver.posts.create_post(options={'channel_id': channel_id, 'message': "\n".join(msg_lines), 'root_id': root_id, 'file_ids': [up_file['file_infos'][0]['id']]})

        # 7. –¢–ï–ì–ò–†–û–í–ê–ù–ò–ï –õ–ò–î–û–í
        if teams_with_issues and leads_mapping:
            leads_to_tag = set()
            for t_name in teams_with_issues:
                if t_name in leads_mapping:
                    leads_to_tag.add(leads_mapping[t_name])

            if leads_to_tag:
                tags_str = ", ".join(sorted(list(leads_to_tag)))
                driver.posts.create_post(options={
                    'channel_id': channel_id,
                    'message': f"–í–Ω–∏–º–∞–Ω–∏–µ: {tags_str} ‚Äî –≤ –≤–∞—à–∏—Ö –∫–æ–º–∞–Ω–¥–∞—Ö –µ—Å—Ç—å —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è.",
                    'root_id': root_id
                })

        print("[THREAD] –ì–æ—Ç–æ–≤–æ!", flush=True)

    except Exception as e:
        print(f"[THREAD] Error: {e}", flush=True)
        update_status_text(f"üí• –û—à–∏–±–∫–∞: {e}")

# --- MATTERMOST HANDLER ---
async def my_event_handler(event):
    try: data = json.loads(event)
    except: return
    if 'event' not in data or data['event'] != 'posted': return
    try: post = json.loads(data['data']['post'])
    except: return
    if post.get('props', {}).get('from_bot') == 'true': return
    if post.get('channel_id') != MM_TARGET_CHANNEL_ID: return

    file_ids = post.get('file_ids', [])
    if len(file_ids) > 0:
        print(f"üì• –§–∞–π–ª –ø–æ–ª—É—á–µ–Ω. –ó–∞–ø—É—Å–∫–∞—é –ø–æ—Ç–æ–∫...", flush=True)
        for file_id in file_ids:
            t = threading.Thread(target=worker_process_file, args=(file_id, post['channel_id'], post['id']))
            t.start()

# --- INIT ---
if __name__ == "__main__":
    print(f"üöÄ –ó–∞–ø—É—Å–∫ (v5.1 Fixed) –¥–ª—è {MM_URL}...", flush=True)
    driver = Driver({'url': MM_URL, 'token': MM_TOKEN, 'scheme': MM_SCHEME, 'port': MM_PORT, 'verify': VERIFY_SSL})
    try:
        JIRA_LOOKUP_CACHE, JIRA_KEY_CACHE = get_all_jira_users()
        driver.login()
        driver.init_websocket(my_event_handler)
    except Exception as e:
        print(f"‚ùå –°–±–æ–π: {e}", flush=True)
        sys.exit(1)
